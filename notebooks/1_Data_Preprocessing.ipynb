{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e7cba4b3",
   "metadata": {},
   "source": [
    "# 1. Data Preprocessing\n",
    "\n",
    "> **Purpose:**  \n",
    "> This notebook guides the user through the preprocessing steps required for the perturbation experiments.  \n",
    "> It takes the raw downloads from **Step 0** and prepares them for further processing using the **Eclipse BaSyx Python SDK**, while also collecting and organizing relevant metadata.\n",
    "\n",
    "**Notes and Requirements**\n",
    "\n",
    "- Ensure that the directory structure created in **Step 0** (`/app/data/raw/sample/`) is preserved.  \n",
    "- Preprocessing scripts assume the presence of `.aasx` and `.pdf` files in each product folder.\n",
    "- The preprocessing pipeline has been tested with a random sample of the current versions of the manufacturers’ AASX data [16.10.2025].  \n",
    "  However, **manufacturers may update their AASX structures or file formats**.  \n",
    "  In such cases, code adjustments may be required to correctly parse or extract the relevant information.\n",
    "\n",
    "\n",
    "## 1.0 Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e52e2048",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "from pathlib import PurePosixPath\n",
    "import shutil\n",
    "import zipfile\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "\n",
    "import csv\n",
    "import json \n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "from PyPDF2 import PdfReader\n",
    "import basyx\n",
    "from basyx.aas import model\n",
    "from basyx.aas.adapter import aasx\n",
    "import xml.etree.ElementTree as ET\n",
    "from collections import defaultdict\n",
    "\n",
    "# relative path inside the container\n",
    "data_path = \"/app/data/\"\n",
    "raw_data_path = os.path.join(data_path, \"raw/sample\") # Contains raw downloaded files \n",
    "processing_path = os.path.join(data_path, \"processed/sample\") # Folder where files are processed\n",
    "metadata_path = os.path.join(processing_path, \"metadata.csv\") # metadata file"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cffc6ad2",
   "metadata": {},
   "source": [
    "## 1.1 Setup Product Folders\n",
    "\n",
    "The product folders are **copied to the processing directory**, and all contained files are **renamed according to the product IDs**.  \n",
    "This ensures consistent naming and avoids conflicts in later processing steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aee61d56",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(processing_path):\n",
    "    shutil.copytree(raw_data_path, processing_path)\n",
    "\n",
    "# rename files to have consistent format\n",
    "for product_id in tqdm(os.listdir(processing_path)):\n",
    "    product_path = os.path.join(processing_path, product_id)\n",
    "    if os.path.isdir(product_path):\n",
    "        for file_name in os.listdir(product_path):\n",
    "            if file_name.endswith(\".pdf\"):\n",
    "                new_file_name = f\"{product_id}.pdf\"\n",
    "                os.rename(os.path.join(product_path, file_name), os.path.join(product_path, new_file_name))\n",
    "            elif file_name.endswith(\".aasx\"):\n",
    "                new_file_name = f\"{product_id}.aasx\"\n",
    "                os.rename(os.path.join(product_path, file_name), os.path.join(product_path, new_file_name))\n",
    "            else:\n",
    "                print(f\"Unknown file type: {file_name} in {product_id}\")\n",
    "                #os.remove(os.path.join(product_path, file_name))  # remove non-pdf and non-aasx files"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08e382cf",
   "metadata": {},
   "source": [
    "## 1.2 Raw Text-Level Preprocessing\n",
    "\n",
    "In this step, the downloaded `*.aasx` files are **normalized and corrected** to ensure they can be successfully parsed using the **BaSyx Python SDK**.\n",
    "\n",
    "### 1.2.1 Decompress `*.aasx` Files\n",
    "\n",
    "Each `*.aasx` archive is unzipped to allow direct access to its internal files.  \n",
    "The AASX format follows the Open Packaging Convention (OPC), meaning it is effectively a structured ZIP container."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b72871fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 15/15 [00:04<00:00,  3.70it/s]\n"
     ]
    }
   ],
   "source": [
    "def decompress_aasx_files(directory='.'):\n",
    "    for filename in os.listdir(directory):\n",
    "        if filename.lower().endswith('.aasx'):\n",
    "            base_name = os.path.splitext(filename)[0]\n",
    "            zip_name = f\"{base_name}.zip\"\n",
    "            folder_name = os.path.join(directory, base_name)\n",
    "            # Rename .aasx to .zip\n",
    "            original_path = os.path.join(directory, filename)\n",
    "            zip_path = os.path.join(directory, zip_name)\n",
    "            os.rename(original_path, zip_path)\n",
    "\n",
    "            # Extract .zip contents\n",
    "            with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
    "                zip_ref.extractall(folder_name)\n",
    "\n",
    "            # Rename .zip back to .aasx\n",
    "            os.rename(zip_path, original_path)\n",
    "\n",
    "for product_id in tqdm(os.listdir(processing_path)):\n",
    "    product_path = os.path.join(processing_path, product_id)\n",
    "    if os.path.isdir(product_path):\n",
    "        decompress_aasx_files(product_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a6c6c32",
   "metadata": {},
   "source": [
    "### 1.2.2 Fix Relationship Files\n",
    "\n",
    "`*.rels` **relationship files** define the internal structure and references between documents inside the AASX container.  \n",
    "This preprocessing step verifies and corrects these relationship definitions to ensure that all AASX files can be read without parsing errors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "669e5660",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "15it [00:01, 12.07it/s]\n"
     ]
    }
   ],
   "source": [
    "# Define the mappings to fix\n",
    "URL_REPLACEMENTS = {\n",
    "    \"http://www.admin-shell.io/aasx/relationships/aasx-origin\":\n",
    "        \"http://admin-shell.io/aasx/relationships/aasx-origin\",\n",
    "    \"http://www.admin-shell.io/aasx/relationships/aas-spec\":\n",
    "        \"http://admin-shell.io/aasx/relationships/aas-spec\",\n",
    "    \"http://www.admin-shell.io/aasx/relationships/aas-spec-split\":\n",
    "        \"http://admin-shell.io/aasx/relationships/aas-spec-split\",\n",
    "    \"http://www.admin-shell.io/aasx/relationships/aas-suppl\":\n",
    "        \"http://admin-shell.io/aasx/relationships/aas-suppl\"\n",
    "}\n",
    "\n",
    "def fix_urls_in_file(file_path):\n",
    "    try:\n",
    "        with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            content = f.read()\n",
    "\n",
    "        original_content = content\n",
    "        for old_url, new_url in URL_REPLACEMENTS.items():\n",
    "            content = content.replace(old_url, new_url)\n",
    "\n",
    "        if content != original_content:\n",
    "            with open(file_path, \"w\", encoding=\"utf-8\") as f:\n",
    "                f.write(content)\n",
    "            print(f\"Updated: {file_path}\")\n",
    "        else:\n",
    "            pass\n",
    "            #print(f\"No changes: {file_path}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {file_path}: {e}\")\n",
    "\n",
    "\n",
    "for product_path in tqdm(Path(processing_path).iterdir()):\n",
    "    if os.path.isdir(product_path):\n",
    "        for rels_file in product_path.rglob(\"*.rels\"):\n",
    "            fix_urls_in_file(rels_file)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "402715ce",
   "metadata": {},
   "source": [
    "### 1.2.3 Collect Metadata\n",
    "\n",
    "During preprocessing, relevant metadata is collected for later analysis.  \n",
    "Specifically, this step:\n",
    "\n",
    "- Checks the **AAS version** (e.g., v2.0, v3.0).  \n",
    "- Identifies the **Technical Data Submodel**, if present."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "86f2c49d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('/app/data/processed/sample/Festo_8062201/Festo_8062201/aasx/AAS_Type_VFFG_T_F6_A_V1/AAS_Type_VFFG_T_F6_A_V1.aas.xml',\n",
       " '/app/data/processed/sample/Festo_8062201/Festo_8062201/aasx/_rels/aasx-origin.rels',\n",
       " '/app/data/processed/sample/Festo_8062201/Festo_8062201/aasx/AAS_Type_VFFG_T_F6_A_V1/_rels/AAS_Type_VFFG_T_F6_A_V1.aas.xml.rels')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def read_rels(folder_path, rels_path):\n",
    "    \"\"\"Parse a .rels file from the extracted AASX folder.\"\"\"\n",
    "    rels_file = Path(folder_path) / rels_path\n",
    "    if not rels_file.exists():\n",
    "        raise FileNotFoundError(f\"Missing .rels file: {rels_file}\")\n",
    "    \n",
    "    xml = ET.parse(rels_file).getroot()\n",
    "    ns = {\"r\": \"http://schemas.openxmlformats.org/package/2006/relationships\"}\n",
    "    rels = []\n",
    "    for rel in xml.findall(\"r:Relationship\", ns):\n",
    "        rels.append({\n",
    "            \"Id\": rel.attrib[\"Id\"],\n",
    "            \"Type\": rel.attrib[\"Type\"],\n",
    "            \"Target\": rel.attrib[\"Target\"],\n",
    "        })\n",
    "    return rels\n",
    "\n",
    "def find_aas_file(folder_path):\n",
    "    \"\"\"Find the main AAS file path in an extracted AASX folder.\n",
    "    Returns:\n",
    "        main_aas_path (str): Path to the main AAS XML/JSON file.\n",
    "        origin_rels_path (str): Path to the rels file pointing to it.\n",
    "        aas_rels_path (str | None): Path to the .rels file next to the AAS file, if it exists.\n",
    "    \"\"\"\n",
    "    folder_path = Path(folder_path)\n",
    "\n",
    "    # 1️⃣ Read package-level relationships\n",
    "    root_rels = read_rels(folder_path, \"_rels/.rels\")\n",
    "    origin_rel = next((r for r in root_rels if \"aasx-origin\" in r[\"Target\"]), None)\n",
    "    if not origin_rel:\n",
    "        raise FileNotFoundError(\"No aasx-origin found in _rels/.rels\")\n",
    "\n",
    "    # 2️⃣ Follow to the next relationship file\n",
    "    origin_rel_path = PurePosixPath(origin_rel[\"Target\"])\n",
    "    if origin_rel_path.is_absolute():\n",
    "        origin_rel_path = PurePosixPath(str(origin_rel_path)[1:])  # strip leading slash\n",
    "\n",
    "    rels_for_origin = f\"{origin_rel_path.parent}/_rels/{origin_rel_path.name}.rels\"\n",
    "    rels_for_origin = str(rels_for_origin).lstrip(\"/\")  # ensure relative\n",
    "    origin_rels = read_rels(folder_path, rels_for_origin)\n",
    "\n",
    "    # 3️⃣ Find the main AAS entry (XML or JSON)\n",
    "    main_rel = next(\n",
    "        (r for r in origin_rels if \"aas\" in r[\"Target\"] or \"content\" in r[\"Target\"]),\n",
    "        None\n",
    "    )\n",
    "    if not main_rel:\n",
    "        raise FileNotFoundError(\"No main AAS relationship found\")\n",
    "\n",
    "    main_target = origin_rel_path.parent / main_rel[\"Target\"]\n",
    "    if main_target.is_absolute():\n",
    "        main_target = str(main_target)[1:]\n",
    "    else:\n",
    "        main_target = str(main_target)\n",
    "\n",
    "    main_aas_path = folder_path / main_target\n",
    "\n",
    "    # 4️⃣ Find the .rels file next to the main AAS file (if it exists)\n",
    "    aas_rels_path = (\n",
    "        main_aas_path.parent / \"_rels\" / f\"{main_aas_path.name}.rels\"\n",
    "    )\n",
    "    if not aas_rels_path.exists():\n",
    "        aas_rels_path = None  # not all AASX files include this\n",
    "\n",
    "    # 5️⃣ Return all three\n",
    "    return str(main_aas_path), str(folder_path / rels_for_origin), str(aas_rels_path) if aas_rels_path else None\n",
    "\n",
    "find_aas_file(os.path.join(processing_path,\"Wago_2000-3228\",\"Wago_2000-3228\"))\n",
    "find_aas_file(os.path.join(processing_path,\"Festo_8062201\",\"Festo_8062201\"))\n",
    "#find_aas_file(os.path.join(processing_path,\"Harting_24024070000\",\"Harting_24024070000\"))\n",
    "#find_aas_file(os.path.join(processing_path,\"RStahl_261385\",\"RStahl_261385\")) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5eb40bc1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 15/15 [00:00<00:00, 22.57it/s]\n"
     ]
    }
   ],
   "source": [
    "# Collect Metadata\n",
    "\n",
    "# Regex to extract version from URL\n",
    "version_pattern = re.compile(r\"https?://(?:www\\.)?admin-shell\\.io/aas/([1-3])/0\")\n",
    "#TODO find version also from *.json files\n",
    "\n",
    "def analyze_file(filepath):\n",
    "    \"\"\"Reads the file and returns (version, has_technical_data)\"\"\"\n",
    "    try:\n",
    "        with open(filepath, \"r\", encoding=\"utf-8\", errors=\"ignore\") as f:\n",
    "            content = f.read()\n",
    "\n",
    "        # Extract version\n",
    "        version_match = version_pattern.search(content)\n",
    "        version = f\"{version_match.group(1)}.0\" if version_match else \"Not Found\"\n",
    "\n",
    "        # Check for 'TechnicalData'\n",
    "        has_technical_data = \"Yes\" if \"TechnicalData\" in content else \"No\"\n",
    "\n",
    "        return (str(filepath), version, has_technical_data)\n",
    "    except Exception as e:\n",
    "        return (str(filepath), f\"Error: {e}\", \"Error\")\n",
    "\n",
    "metadata = []\n",
    "for product_id in tqdm(os.listdir(processing_path)):\n",
    "    if not os.path.isdir(os.path.join(processing_path, product_id)) or product_id == \"configs\":\n",
    "        continue\n",
    "    product_path = os.path.join(processing_path, product_id, product_id)\n",
    "    aas_file, _, _ = find_aas_file(product_path)\n",
    "    if not os.path.exists(aas_file):\n",
    "        print(f\"AAS file not found for {product_id}\")\n",
    "        continue\n",
    "    aas_path, aas_version, has_technical_data = analyze_file(aas_file)\n",
    "    metadata.append({\"Product_Id\": product_id, \"AAS_File\": aas_path, \"AAS_Version\": aas_version, \"Has_Technical_Data\": has_technical_data})\n",
    "metadata_df = pd.DataFrame(metadata)\n",
    "metadata_df.to_csv(metadata_path, index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e9c0edda",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Move datasheets to product folder\n",
    "# Currently only for RStahl AASX files contain the product datasheets\n",
    "# Uses a specific regex to identify the correct datasheet\n",
    "\n",
    "\n",
    "no_datasheet = []\n",
    "en_pattern = re.compile(r'V\\s*[\\d\\.]+\\s+EN')\n",
    "\n",
    "for product_id in os.listdir(processing_path):\n",
    "    if not product_id.startswith(\"RStahl\"):\n",
    "        continue\n",
    "    file_versions = {}\n",
    "\n",
    "    product_files_path = os.path.join(processing_path, product_id, product_id, \"aasx\", \"files\")\n",
    "\n",
    "    for file in os.listdir(product_files_path):\n",
    "        if file.startswith(\"ZDB\") and file.endswith(\".pdf\"):\n",
    "            file_path = os.path.join(product_files_path, file)\n",
    "            reader = PdfReader(file_path)\n",
    "\n",
    "            highest_version = None\n",
    "\n",
    "            for page in reader.pages[:1]:\n",
    "                text = page.extract_text()\n",
    "                if text:\n",
    "                    matches = en_pattern.findall(text)\n",
    "                    if matches:\n",
    "                        # Assuming version appears once per page, take first match\n",
    "                        version_str = matches[0]\n",
    "                        try:\n",
    "                            version = float(version_str)\n",
    "                        except ValueError:\n",
    "                            version = -1  # fallback if version string can't be converted\n",
    "                        highest_version = version  # Keep latest found version in document\n",
    "\n",
    "            if highest_version is not None:\n",
    "                file_versions[file] = highest_version\n",
    "\n",
    "    if not file_versions:\n",
    "        no_datasheet.append(product_id) # should be empty\n",
    "    else:\n",
    "        if len(file_versions) == 1:\n",
    "            selected_file = list(file_versions.keys())[0]\n",
    "        else:\n",
    "            # Select file with highest version\n",
    "            selected_file = max(file_versions.items(), key=lambda x: x[1])[0]\n",
    "        # Move selected file to product folder\n",
    "        selected_file_path = os.path.join(product_files_path, selected_file)\n",
    "        product_folder = os.path.join(processing_path, product_id)\n",
    "        new_file_path = os.path.join(product_folder, f\"{product_id}.pdf\")\n",
    "        shutil.copy(selected_file_path, new_file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49f59e12",
   "metadata": {},
   "source": [
    "### 1.2.4 Remove Unnecessary Files\n",
    "\n",
    "Files that **inflate the AASX size** and are **not required for subsequent processing** are removed.  \n",
    "References to these files are also deleted to avoid broken links or parsing errors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "99f8d908",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove files to save space (RStahl datasheets are now in product folder)\n",
    "for product_id in os.listdir(processing_path):\n",
    "    if not product_id.startswith(\"RStahl\"):\n",
    "        continue\n",
    "    product_files_path = os.path.join(processing_path, product_id, product_id, \"aasx\", \"files\")\n",
    "    if os.path.exists(product_files_path):\n",
    "        for file in os.listdir(product_files_path):\n",
    "            if file.endswith(\".pdf\"):\n",
    "                file_path = os.path.join(product_files_path, file)\n",
    "                os.remove(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6cb90b8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 16/16 [00:00<00:00, 29.92it/s]\n"
     ]
    }
   ],
   "source": [
    "# Remove CAD files to avoid license issues\n",
    "for product_id in tqdm(os.listdir(processing_path)):\n",
    "    if not os.path.isdir(os.path.join(processing_path,product_id)):\n",
    "        continue\n",
    "    if not product_id.startswith(\"Festo\"):\n",
    "        continue\n",
    "\n",
    "    product_files_path = os.path.join(processing_path, product_id, product_id, \"aasx\")\n",
    "    files_to_delete = []\n",
    "    for file in Path(product_files_path).rglob(\"*\"):\n",
    "        if str(file).endswith(\".zip\") or str(file).endswith(\".edz\"):\n",
    "            file_path = os.path.join(product_files_path, file)\n",
    "            files_to_delete.append(file)\n",
    "            os.remove(file)\n",
    "\n",
    "    aas_path, _, rels_path = find_aas_file(os.path.join(processing_path, product_id, product_id))\n",
    "\n",
    "    # remove references to deleted files in the .rels file\n",
    "    with open(rels_path, 'r', encoding='utf-8') as file:\n",
    "        rels_content = file.read()\n",
    "    for filename in files_to_delete:\n",
    "        pattern = rf'<Relationship\\s[^>]*Target=\"[^\"]*{re.escape(filename.name)}\"[^>]*/>'\n",
    "        rels_content = re.sub(pattern, '', rels_content)\n",
    "    with open(rels_path, 'w', encoding='utf-8') as file:\n",
    "        file.write(rels_content)\n",
    "\n",
    "    # remove references to deleted files in the main AAS file\n",
    "    with open(aas_path, 'r', encoding='utf-8') as file:\n",
    "        aas_content = file.read()\n",
    "    for filename in files_to_delete:\n",
    "        pattern = rf'/aasx(?:/[^/\\s]+)*/{re.escape(filename.name)}'\n",
    "        aas_content = re.sub(pattern, '', aas_content)\n",
    "\n",
    "    # save the cleaned AAS file\n",
    "    with open(aas_path, 'w', encoding='utf-8') as file:\n",
    "        file.write(aas_content)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ad46efc9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 16/16 [00:00<00:00, 48.94it/s]\n"
     ]
    }
   ],
   "source": [
    "def remove_missing_file_urls(product_id,xml_path):\n",
    "    ns = {'aas': 'https://admin-shell.io/aas/3/0'} # assume verson 3.0 only\n",
    "    tree = ET.parse(xml_path)\n",
    "    root = tree.getroot()\n",
    "    # Iterate over all <aas:file> elements\n",
    "    for file_elem in root.findall('.//aas:file', ns):\n",
    "        # Find the <aas:value> element\n",
    "        value_elem = file_elem.find('aas:value', ns)\n",
    "        if value_elem is not None:\n",
    "            if value_elem.text is not None:\n",
    "                full_path = os.path.join(processing_path, product_id, product_id, value_elem.text[1:])\n",
    "                if not os.path.isfile(full_path):\n",
    "                    print(f\"File not found: {full_path}\")\n",
    "                    value_elem.text = ''\n",
    "                    tree.write(xml_path, encoding='UTF-8', xml_declaration=True)\n",
    "\n",
    "\n",
    "for product_id in tqdm(os.listdir(processing_path)):\n",
    "    if not os.path.isdir(os.path.join(processing_path,product_id)):\n",
    "        continue\n",
    "    if not product_id.startswith(\"Harting\"):\n",
    "        continue\n",
    "    # folder_path = os.path.join(harting_path, product_id, product_id, \"aasx\")\n",
    "    # folder_path = Path(folder_path)\n",
    "    # files = folder_path.rglob(\"*.xml\")\n",
    "    file_path = find_aas_file(os.path.join(processing_path, product_id, product_id))[0]\n",
    "    remove_missing_file_urls(product_id, file_path)\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5b58749",
   "metadata": {},
   "source": [
    "### 1.2.4 Clean AAS Files\n",
    " - Merge General Technical Data Submodel Element Collection\n",
    " - Merge or Delete Properties with duplicate idShort\n",
    " - Change all idShorts to only contain alphanumeric digits and \"_\"\n",
    " - Replace German decimal \",\" with \".\"\n",
    " - Lowercase all language tags"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a432268c",
   "metadata": {},
   "source": [
    "### 1.2.5 Clean AAS Files\n",
    "\n",
    "Each AAS file is standardized to improve downstream compatibility:\n",
    "\n",
    "- Merge duplicate **General Technical Data** submodel and **Submodel Element Collections**.  \n",
    "- Merge or delete **properties with duplicate `idShort`**.  \n",
    "- Replace all invalid characters in `idShort` fields so they only contain **alphanumeric characters** and underscores (`_`).  \n",
    "- Convert **German decimal commas (`,`)** to **periods (`.`)** for numeric values.  \n",
    "- Convert all **language tags to lowercase**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2edb0ca5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_general_technical_data(xml_content):\n",
    "    \"\"\"\n",
    "    Some AAS files contain two \"GeneralTechnicalData\" blocks which cannot be handled by basyx.\n",
    "    This function merges these two blocks into one.\n",
    "    \"\"\"\n",
    "    pattern = re.compile(\n",
    "        r'(<submodelElementCollection>\\s*<idShort>GeneralTechnicalData</idShort>.*?<value>.*?</value>\\s*</submodelElementCollection>)',\n",
    "        re.DOTALL\n",
    "    )\n",
    "    matches = pattern.findall(xml_content)\n",
    "\n",
    "    if len(matches) <= 1:\n",
    "        return xml_content  # Nothing to merge\n",
    "\n",
    "    # Identify preferred block: the one with \"Allgemeine Technische Daten\"\n",
    "    preferred_block = None\n",
    "    other_block = None\n",
    "\n",
    "    for block in matches:\n",
    "        if \"Allgemeine Technische Daten\" in block:\n",
    "            preferred_block = block\n",
    "        else:\n",
    "            other_block = block\n",
    "\n",
    "    if not preferred_block or not other_block:\n",
    "        raise ValueError(\"Unexpected XML Structure: Can't identify both blocks.\")\n",
    "\n",
    "    # Extract properties from both blocks\n",
    "    value_pattern = re.compile(\n",
    "        r'<value>\\s*((?:\\s*<(property|multiLanguageProperty)>.*?</\\2>\\s*)+)</value>',\n",
    "        re.DOTALL\n",
    "    )\n",
    "\n",
    "    preferred_value = value_pattern.search(preferred_block).group(1)\n",
    "    other_value = value_pattern.search(other_block).group(1)\n",
    "\n",
    "    merged_value = preferred_value + other_value\n",
    "    merged_block = preferred_block.replace(preferred_value,merged_value)\n",
    "    xml_content = xml_content.replace(preferred_block,merged_block)\n",
    "    xml_content = xml_content.replace(other_block, \"\")\n",
    "    return xml_content\n",
    "\n",
    "\n",
    "def merge_duplicate_properties(xml_text):\n",
    "    \"\"\"\n",
    "    Some submodels contain duplicate properties which cannot be handled by basyx.\n",
    "    This function merges these duplicate properties into one, combining their values or removing duplicates.\"\"\"\n",
    "    # Match property or multiLanguageProperty blocks\n",
    "    pattern = re.compile(\n",
    "        r'(?P<block><(?P<tag>property|multiLanguageProperty).*?</\\2>)',\n",
    "        re.DOTALL\n",
    "    )\n",
    "\n",
    "    # Group by idShort\n",
    "    grouped = defaultdict(list)\n",
    "\n",
    "    # These properties are manually identified to be merged or removed\n",
    "    # This could possibly be done more elegantly\n",
    "    # Possible improvements:\n",
    "    # - Automatically identify properties with duplicate idShort\n",
    "    # - Split timeconstant into Electric, Thermal\n",
    "    # - Split range properties into min and max\n",
    "    properties_to_merge = ['Assembly', 'TypeOfPistonRod', 'ModelOfRotatingElectricalMachines_AccordingToCodeI',\n",
    "                           'Certificate_Approval', 'Certification', 'AmbientTemperature',\n",
    "                           'ControllerFunction', 'PneumaticOutputPort','PneumaticInputPort',\n",
    "                           'PneumaticPilotPort','PilotMedium', 'OperatingMedium', 'PressureMedium', 'CompressedAirQualityClassAtInlet',\n",
    "                           'DegreeOfProtection_IP_Mounted',\n",
    "                           'DesignOfTheElectricalConnection_head2_','DesignOfTheElectricalConnection_head1_', 'FieldBus_system',\n",
    "                           'Markings','InterfaceDesign','AnalogOutputVoltage', 'AnalogOutput', 'IO_LinkDeviceProfile', 'SupportedProtocol',\n",
    "                           'MaterialOfHousing', 'HousingMaterial', 'TypeOfAdjustment', 'StyleOfCommunicationInterface', 'ConductorConnectionMethod','TypeOfConnector',\n",
    "                           'SignalStatusDisplay', 'DegreeOfProtection', 'ManualOverride', 'TypeOfPneumaticConnections', 'IPProtectionClassWithConnector',\n",
    "                           'ConnectionType', 'DesignOfTheElectricalConnection', 'ThreadSizeConnector', 'Coding', 'EncoderProtocol', 'DesignOfFeedbackSystem',\n",
    "                           'ModeOfOperationEnd_PositionLocking', 'TypeOfPistonRodEnd', 'Labs_ConformityToVDMA24364','TypeOfCushioning', 'PistonRodThread',\n",
    "                           'CondensateDrain', 'CompressedAirQualityClassAtOutlet', 'ArrangementOfTheCableLead_In_head2_', 'TypeOfPlug_InContactHead1',\n",
    "                           'DesignOfTheProcessConnection','Lap', 'DirectionalControlValveFunction', 'ControlCharacteristics', 'FunctionInNormalPosition',\n",
    "                           'MountingOrientation', 'ValveReturn', 'OperatingVoltageType', 'MeasurementMethodForPressureFlow','IPProtectionClassWithoutConnector', 'PneumaticExhaustPort']\n",
    "    properties_max_one = ['MaxWorkingPressure','PressureRegulationRange','OperatingPressure','MaxPressureHysteresis', 'MaximumOutputPressure',\n",
    "                            'MinimumOutputPressure','MaximumOperatingPressure','MinimumOperatingPressure', 'MinimumPilotPressure','MaximumPilotPressure']\n",
    "    properties_to_delete = ['TimeConstant']\n",
    "\n",
    "    for match in pattern.finditer(xml_text):\n",
    "        block = match.group('block')\n",
    "        tag = match.group('tag')\n",
    "        idshort_match = re.search(r'<idShort>(.*?)</idShort>', block)\n",
    "        if idshort_match.group(1) not in properties_to_merge + properties_max_one + properties_to_delete:\n",
    "            continue\n",
    "\n",
    "        # Parse the XML fragment inside a root element\n",
    "        root = ET.fromstring(f\"<root>{block}</root>\")\n",
    "        content = {}\n",
    "        if tag == 'property':\n",
    "            # Find the <property> element\n",
    "            property_elem = root.find('property')\n",
    "            # Loop through direct children to find the correct <value>\n",
    "            for child in property_elem:\n",
    "                if child.tag == 'value':\n",
    "                    content['de'] = child.text.strip()\n",
    "        elif tag == 'multiLanguageProperty':\n",
    "            # Find the multiLanguageProperty block\n",
    "            ml_prop = root.find('multiLanguageProperty')\n",
    "            # Navigate to value > langStringTextType\n",
    "            for lang_entry in ml_prop.find('value').findall('langStringTextType'):\n",
    "                lang = lang_entry.find('language').text.strip()\n",
    "                text = lang_entry.find('text').text.strip()\n",
    "                content[lang] = text\n",
    "\n",
    "        if idshort_match:\n",
    "            idshort = idshort_match.group(1)\n",
    "            grouped[idshort].append([match.start(), match.end(), block, tag, content])\n",
    "    ops = []\n",
    "    for dprop in properties_max_one:\n",
    "        if dprop in grouped:\n",
    "            for i in grouped[dprop][1:]:\n",
    "                ops.append((i[0],i[1],''))\n",
    "    for dprop in properties_to_delete:\n",
    "        if dprop in grouped:\n",
    "            for i in grouped[dprop]:\n",
    "                ops.append((i[0],i[1],''))\n",
    "    for mprop in properties_to_merge:\n",
    "        if mprop in grouped:\n",
    "            for i in grouped[mprop][1:]:\n",
    "                if 'de' in i[-1]:\n",
    "                    grouped[mprop][0][2] = grouped[mprop][0][2].replace(grouped[mprop][0][-1]['de'], i[-1]['de'] + \"\\n\" +grouped[mprop][0][-1]['de'])\n",
    "                    grouped[mprop][0][-1]['de'] = i[-1]['de']+ '\\n' + grouped[mprop][0][-1]['de']\n",
    "                if 'en' in i[-1] and 'en' in grouped[mprop][0][-1]:\n",
    "                    grouped[mprop][0][2] = grouped[mprop][0][2].replace(grouped[mprop][0][-1]['en'], i[-1]['en'] + \"\\n\" +grouped[mprop][0][-1]['en'])\n",
    "                    grouped[mprop][0][-1]['en'] = i[-1]['en']+ '\\n' + grouped[mprop][0][-1]['en']\n",
    "            for i in grouped[mprop][1:][::-1]:\n",
    "                ops.append((i[0],i[1],''))\n",
    "            ops.append((grouped[mprop][0][0],grouped[mprop][0][1], grouped[mprop][0][2]))\n",
    "    ops.sort(key=lambda x: x[0], reverse=True)\n",
    "    for start, end, text in ops:\n",
    "        xml_text = xml_text[:start] + text + xml_text[end:]\n",
    "    return xml_text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c935d6cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 16/16 [00:00<00:00, 52.02it/s]\n"
     ]
    }
   ],
   "source": [
    "for product_id in tqdm(os.listdir(processing_path)):\n",
    "    if not os.path.isdir(os.path.join(processing_path,product_id)):\n",
    "        continue\n",
    "    if not product_id.startswith(\"Festo\"):\n",
    "        # TODO: scan all manufacuterers for the following issues\n",
    "        continue\n",
    "    aas_path, _, _ = find_aas_file(os.path.join(processing_path, product_id, product_id))\n",
    "    with open(aas_path, 'r', encoding='utf-8') as file:\n",
    "        aas_content = file.read()\n",
    "\n",
    "\n",
    "    # german floats with comma\n",
    "    pattern = rf'(?<=\\d),(?=\\d)'\n",
    "    aas_content = re.sub(pattern, '.', aas_content)\n",
    "    # replace all idShort with only alphanumeric characters and underscores\n",
    "    aas_content = re.sub(r'<idShort>([^<]*)</idShort>', lambda m: f\"<idShort>{re.sub(r'[^a-zA-Z0-9_]', '_', m.group(1))}</idShort>\", aas_content)\n",
    "    # merge duplicate properties\n",
    "\n",
    "    aas_content = merge_duplicate_properties(aas_content)\n",
    "    aas_content = merge_general_technical_data(aas_content)\n",
    "\n",
    "    with open(aas_path, 'w', encoding='utf-8') as file:\n",
    "        file.write(aas_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "aef79607",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fix_language_tags_in_dict(obj):\n",
    "    \"\"\"\n",
    "    Recursively correct 'value' fields with language tags to lowercase.\n",
    "    \"\"\"\n",
    "    if isinstance(obj, list):\n",
    "        for item in obj:\n",
    "            fix_language_tags_in_dict(item)\n",
    "    elif isinstance(obj, dict):\n",
    "        for key, value in obj.items():\n",
    "            if isinstance(value, dict):\n",
    "                fix_language_tags_in_dict(value)\n",
    "            elif isinstance(value, list):\n",
    "                for item in value:\n",
    "                    fix_language_tags_in_dict(item)\n",
    "            elif isinstance(value, str) and len(value) == 2 and key == \"language\":\n",
    "                # If the value is a language tag, convert it to lowercase\n",
    "                obj[key] = value.lower()\n",
    "                #print(f\"Corrected language tag: {value} to {obj[key]}\")\n",
    "                \n",
    "            if 'idShort' in obj and obj['idShort'].startswith(\"Language\") and key == \"value\":\n",
    "                obj[key] = value.lower()\n",
    "                #print(f\"Corrected language tag: {value} to {obj[key]}\")\n",
    "\n",
    "\n",
    "def fix_json_file(file_path):\n",
    "    \"\"\"\n",
    "    Load JSON, fix language tags, and overwrite the file.\n",
    "    \"\"\"\n",
    "    with open(file_path, encoding=\"utf-8-sig\") as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    fix_language_tags_in_dict(data)\n",
    "\n",
    "    with open(file_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(data, f, ensure_ascii=False, indent=2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1f05bc11",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 16/16 [00:00<00:00, 19.96it/s]\n"
     ]
    }
   ],
   "source": [
    "for product_id in tqdm(os.listdir(processing_path)):\n",
    "    if not os.path.isdir(os.path.join(processing_path,product_id)):\n",
    "        continue\n",
    "    if not product_id.startswith(\"RStahl\"):\n",
    "        continue\n",
    "    json_file_path = find_aas_file(os.path.join(processing_path, product_id, product_id))[0]\n",
    "    fix_json_file(json_file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a38c991c",
   "metadata": {},
   "source": [
    "### 1.2.6 Compress to AASX Archive Again\n",
    "\n",
    "After cleaning and validation, each directory is **zipped back into a `.aasx` archive**, and the temporary unzipped folders are removed.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1af3cb17",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 16/16 [00:08<00:00,  1.95it/s]\n"
     ]
    }
   ],
   "source": [
    "# compress and rename to product_id.aasx\n",
    "for product_id in tqdm(os.listdir(processing_path)):\n",
    "    folder_path = os.path.join(processing_path, product_id, product_id)\n",
    "    aasx_path = os.path.join(processing_path, product_id, product_id+\".aasx\")\n",
    "    if os.path.exists(folder_path):\n",
    "        # Create the ZIP (aasx) file\n",
    "        with zipfile.ZipFile(aasx_path, 'w', compression=zipfile.ZIP_DEFLATED, compresslevel=9) as zf:\n",
    "            # Walk the folder and add files\n",
    "            for root, _, files in os.walk(folder_path):\n",
    "                for file in files:\n",
    "                    full_path = os.path.join(root, file)\n",
    "                    # Compute the archive name (relative path inside the ZIP)\n",
    "                    arcname = os.path.relpath(full_path, start=folder_path)\n",
    "                    zf.write(full_path, arcname)\n",
    "        shutil.rmtree(folder_path)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1425563c",
   "metadata": {},
   "source": [
    "## 1.3 Object-Level Preprocessing\n",
    "\n",
    "At this stage, the cleaned `*.aasx` files can be processed using the **BaSyx Python SDK** to extract individual submodels and gather additional metadata.\n",
    "\n",
    "### 1.3.1 Extract Technical Data Submodels\n",
    "\n",
    "The **TechnicalData** submodels are extracted from each AAS and saved in **JSON format** for downstream processing and analysis.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38836f00",
   "metadata": {},
   "outputs": [],
   "source": [
    "for product_id in tqdm(os.listdir(processing_path)):\n",
    "    if not os.path.isdir(os.path.join(processing_path,product_id)):\n",
    "        continue\n",
    "\n",
    "    object_store = model.DictObjectStore()\n",
    "    file_store = aasx.DictSupplementaryFileContainer()    \n",
    "    with aasx.AASXReader(os.path.join(processing_path,product_id, product_id+\".aasx\")) as reader:\n",
    "        # Read all contained AAS objects and all referenced auxiliary files\n",
    "        reader.read_into(object_store=object_store,\n",
    "                         file_store=file_store)\n",
    "    for i in object_store:\n",
    "        if i.id_short == \"TechnicalData\":\n",
    "            technical_data_url = i.id\n",
    "            break\n",
    "    else:\n",
    "        raise ValueError(f\"TechnicalData submodel not found for {product_id}\")\n",
    "    \n",
    "    submodel = object_store.get_identifiable(technical_data_url)\n",
    "\n",
    "    with open(os.path.join(processing_path,product_id,product_id+\"_technical_data.json\"), \"w\") as file:\n",
    "        json.dump(submodel, file, cls=basyx.aas.adapter.json.AASToJsonEncoder)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da45ef83",
   "metadata": {},
   "source": [
    "### 1.3.2 Collect Metadata\n",
    "\n",
    "For each processed product, the following metadata is collected:\n",
    "\n",
    "- **Product Classification** according to the ECLASS system (version and identifier)  \n",
    "- **Number of technical properties** present in the TechnicalData submodel\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1039e122",
   "metadata": {},
   "outputs": [],
   "source": [
    "def walk_elements(element):\n",
    "    \"\"\"Recursively yield all SubmodelElements.\"\"\"\n",
    "    if isinstance(element, model.SubmodelElementCollection):\n",
    "        for child in element.value:\n",
    "            yield from walk_elements(child)\n",
    "    else:\n",
    "        yield element\n",
    "\n",
    "def normalize_name(name: str):\n",
    "    \"\"\"Normalize element names for easier matching.\"\"\"\n",
    "    return name.strip().lower() if name else \"\"\n",
    "\n",
    "def parse_class_id(raw_value: str):\n",
    "    \"\"\"\n",
    "    Parse a class ID like '27-44-02-17 | 0173-1#01-AFR572#009'\n",
    "    into (normalized_id, irdi).\n",
    "    \"\"\"\n",
    "    parts = [p.strip() for p in raw_value.split(\"|\")]\n",
    "    class_id = parts[0].replace(\"-\", \"\").strip()\n",
    "    irdi = parts[1] if len(parts) > 1 else None\n",
    "    return class_id, irdi\n",
    "\n",
    "def looks_like_eclass(class_id: str):\n",
    "    \"\"\"Heuristic to detect if a class ID pattern resembles an ECLASS code.\"\"\"\n",
    "    if not class_id:\n",
    "        return False\n",
    "    if re.match(r\"^\\d{8}$\", class_id):  # e.g., 51030401\n",
    "        return True\n",
    "    if re.match(r\"^\\d{2}-\\d{2}-\\d{2}-\\d{2}$\", class_id):  # e.g., 27-44-02-17\n",
    "        return True\n",
    "    if \"|\" in class_id and \"0173-1#\" in class_id:\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "def normalize_version(version: str):\n",
    "    \"\"\"Normalize ECLASS version strings like '12.0 (BASIC)' or '12' → '12.0'.\"\"\"\n",
    "    if not version:\n",
    "        return \"\"\n",
    "    version = version.strip()\n",
    "    # Remove things like (BASIC), (ADVANCED), etc.\n",
    "    version = re.sub(r\"\\s*\\([^)]*\\)\", \"\", version).strip()\n",
    "    # If it's a plain integer, add .0\n",
    "    if re.fullmatch(r\"\\d+\", version):\n",
    "        version = f\"{version}.0\"\n",
    "    return version\n",
    "\n",
    "def extract_classifications(submodel):\n",
    "    \"\"\"Extract possible classification triplets from a TechnicalData submodel.\"\"\"\n",
    "    results = []\n",
    "    current = {}\n",
    "    \n",
    "    for elem in walk_elements(submodel):\n",
    "        name = normalize_name(elem.id_short)\n",
    "        value = str(getattr(elem, \"value\", \"\")).strip()\n",
    "        if not value:\n",
    "            continue\n",
    "\n",
    "        if \"systemversion\" in name:\n",
    "            current[\"version\"] = value\n",
    "        elif \"system\" in name:\n",
    "            current[\"system\"] = value\n",
    "        elif \"classid\" in name:\n",
    "            current[\"class_id_raw\"] = value\n",
    "\n",
    "        if all(k in current for k in (\"system\", \"version\", \"class_id_raw\")):\n",
    "            results.append(current)\n",
    "            current = {}\n",
    "\n",
    "    if current and \"class_id_raw\" in current:\n",
    "        results.append(current)\n",
    "\n",
    "    return results\n",
    "\n",
    "def get_eclass_info(submodel):\n",
    "    \"\"\"Return the ECLASS classification info (system, version, class_id, irdi).\"\"\"\n",
    "    classifications = extract_classifications(submodel)\n",
    "    if not classifications:\n",
    "        return None\n",
    "\n",
    "    # Filter for ECLASS\n",
    "    eclass_entries = [\n",
    "        c for c in classifications if \"eclass\" in c.get(\"system\", \"\").lower()\n",
    "    ]\n",
    "\n",
    "    # If no explicit ECLASS but only one system, infer from pattern\n",
    "    if not eclass_entries and len(classifications) == 1:\n",
    "        c = classifications[0]\n",
    "        class_id_raw = c.get(\"class_id_raw\", \"\")\n",
    "        if looks_like_eclass(class_id_raw):\n",
    "            c[\"system\"] = \"ECLASS\"\n",
    "            eclass_entries = [c]\n",
    "\n",
    "    if not eclass_entries:\n",
    "        return None\n",
    "\n",
    "    chosen = eclass_entries[-1]  # choose last or most complete one\n",
    "    class_id_raw = chosen.get(\"class_id_raw\", \"\")\n",
    "    class_id, irdi = parse_class_id(class_id_raw)\n",
    "    version = normalize_version(chosen.get(\"version\", \"\"))\n",
    "\n",
    "    result = {\n",
    "        \"Classification_System\": \"ECLASS\",\n",
    "        \"Classification_System_Version\": version,\n",
    "        \"Class_Id\": class_id,\n",
    "        \"IRDI\": irdi\n",
    "    }\n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eedc9d99",
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata = []\n",
    "for product_id in tqdm(os.listdir(processing_path)):\n",
    "    if not os.path.isdir(os.path.join(processing_path, product_id)) or product_id==\"configs\":\n",
    "        continue\n",
    "    object_store = model.DictObjectStore()\n",
    "    file_store = aasx.DictSupplementaryFileContainer()    \n",
    "    with aasx.AASXReader(os.path.join(processing_path,product_id, product_id+\".aasx\")) as reader:\n",
    "        # Read all contained AAS objects and all referenced auxiliary files\n",
    "        reader.read_into(object_store=object_store,\n",
    "                        file_store=file_store)\n",
    "    for i in object_store:\n",
    "        if i.id_short == \"TechnicalData\":\n",
    "            technical_data_url = i.id\n",
    "            break\n",
    "    else:\n",
    "        raise ValueError(f\"TechnicalData submodel not found for {product_id}\")\n",
    "\n",
    "\n",
    "    submodel = object_store.get_identifiable(technical_data_url)\n",
    "    technical_properties = submodel.get_referable(\"TechnicalProperties\")\n",
    "    properties = []\n",
    "    def extract_properties(elements):\n",
    "        for elem in elements:\n",
    "            if isinstance(elem, model.submodel.SubmodelElementCollection):\n",
    "                extract_properties(elem)  # Recursive call for nested collections\n",
    "            elif isinstance(elem, model.submodel.MultiLanguageProperty) or isinstance(elem, model.submodel.Property):\n",
    "                properties.append(elem)\n",
    "            else:\n",
    "                print(\"Unknown Element\", elem)\n",
    "    extract_properties(technical_properties)\n",
    "\n",
    "\n",
    "    data = get_eclass_info(submodel.get_referable(\"ProductClassifications\"))\n",
    "\n",
    "    data[\"Product_Id\"] = product_id\n",
    "    data[\"Company\"] = product_id.split(\"_\")[0]\n",
    "    data['n_Properties'] = len(properties)\n",
    "    for i in submodel.get_referable(\"GeneralInformation\"):\n",
    "        if i.id_short == \"ManufaturerArticleNumber\":\n",
    "            data['Article_Number'] = submodel.get_referable(\"GeneralInformation\").get_referable(\"ManufacturerArticleNumber\").value\n",
    "    metadata.append(data)\n",
    "\n",
    "new_metadata_df = pd.DataFrame(metadata)\n",
    "metadata_df = pd.read_csv(metadata_path)\n",
    "metadata_df = pd.merge(metadata_df, new_metadata_df, on=\"Product_Id\", how=\"left\")\n",
    "metadata_df.to_csv(metadata_path, index=False)\n",
    "metadata_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5212f6d0",
   "metadata": {},
   "source": [
    "## 1.4 Miscellaneous Preprocessing Code\n",
    "\n",
    "This section contains auxiliary code.\n",
    "\n",
    "### 1.4.1 Retrieve Product URLs\n",
    "\n",
    "Helper routines are provided to retrieve the **source URLs** of each product.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20d59b17",
   "metadata": {},
   "outputs": [],
   "source": [
    "def company_from_product_id(product_id):\n",
    "    return(product_id.split(\"_\")[0])\n",
    "\n",
    "metadata_df = pd.read_csv(os.path.join(processing_path, \"metadata.csv\"))\n",
    "for product_id in os.listdir(processing_path):\n",
    "    if not os.path.isdir(os.path.join(processing_path,product_id)):\n",
    "        continue\n",
    "    company = product_id.split(\"_\")[0]\n",
    "    product_id = \"_\".join(product_id.split(\"_\",1)[1:])\n",
    "    if company == \"Wago\":\n",
    "        url = f\"http://www.wago.com/global/p/{product_id}\"\n",
    "    elif company == \"Harting\":\n",
    "        url = f\"https://www.harting.com/{product_id}\"\n",
    "    elif company == \"RStahl\":\n",
    "        url = f\"https://dt.r-stahl.com/type/{product_id}\"\n",
    "    elif company == \"Festo\":\n",
    "        url = f\"https://www.festo.com/de/en/a/{product_id}\"\n",
    "    print(url)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
