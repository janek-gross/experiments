{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "561627b2",
   "metadata": {},
   "source": [
    "# 3. Information Extraction\n",
    "\n",
    "> **Purpose:**  \n",
    "> This notebook contains the experiments using the **pdf2aas** pipeline to extract **technical properties** from PDF product datasheets.  \n",
    "> For each product, the extraction process is executed multiple times under various **degradation conditions** to assess robustness and reproducibility.\n",
    "\n",
    "## 3.0 Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "43506e83",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /root/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
      "[nltk_data]     /root/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger_eng is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting en-core-web-sm==3.8.0\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.8.0/en_core_web_sm-3.8.0-py3-none-any.whl (12.8 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m12.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable.It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('en_core_web_sm')\n",
      "\u001b[38;5;3m⚠ Restart to reload dependencies\u001b[0m\n",
      "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
      "order to load all the package's dependencies. You can do this by selecting the\n",
      "'Restart kernel' or 'Restart runtime' option.\n"
     ]
    }
   ],
   "source": [
    "from pdf2aas.dictionary import Dictionary, CDD, ECLASS, ETIM\n",
    "from pdf2aas.extractor import PropertyLLM, PropertyLLMSearch, CustomLLMClientHTTP, CustomLLMClient\n",
    "from pdf2aas import preprocessor\n",
    "from openai import OpenAI\n",
    "from pdf2aas.generator import AASSubmodelTechnicalData, AASTemplate\n",
    "from pdf2aas.model import PropertyDefinition, Property\n",
    "import os\n",
    "import json\n",
    "import hashlib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from typing import List, Optional, Union\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "from datetime import datetime\n",
    "from text_perturbation import degrade_prompt\n",
    "import subprocess\n",
    "import glob\n",
    "from time import sleep\n",
    "\n",
    "\n",
    "# relative path inside the container\n",
    "data_path = \"/app/data/\"\n",
    "processing_path = os.path.join(data_path, \"processed/sample\") # Folder where files are processed\n",
    "metadata_path = os.path.join(processing_path, \"metadata.csv\") # metadata file\n",
    "log_path = os.path.join(processing_path, \"experiment_log.csv\")\n",
    "configs_path = os.path.join(processing_path, \"configs\")\n",
    "if not os.path.exists(configs_path):\n",
    "    os.makedirs(configs_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32affe13",
   "metadata": {},
   "source": [
    "## 3.1 Customized Classes\n",
    "\n",
    "This section defines customized helper classes and configurations used to extend or modify default components of the `pdf2aas` pipeline.\n",
    "\n",
    "### 3.1.1 Custom Ollama Client for Open Source LLMs\n",
    "\n",
    "A custom client interface is implemented to interact with open-source Large Language Models (LLMs) via **Ollama**.\n",
    "This class adapts the standard API calls and allows for local model inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "aeb599fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from copy import deepcopy\n",
    "class CustomOllamaClient(CustomLLMClient):\n",
    "    def __init__(\n",
    "        self,\n",
    "        endpoint: str = \"http://host.docker.internal:50000/api/chat\", # url to access host machine from inside the container\n",
    "        result_path: Optional[str] = \"message.content\",\n",
    "        headers: Optional[dict] = None,\n",
    "        timeout: float = 30000, # long timeout for large models\n",
    "        retries: int = 0,\n",
    "    ):\n",
    "        self.endpoint = endpoint\n",
    "        self.result_path = result_path\n",
    "        self.headers = headers or {\n",
    "            \"Content-Type\": \"application/json\",\n",
    "            \"Accept\": \"application/json\",\n",
    "        }\n",
    "        self.timeout = timeout\n",
    "        self.retries = retries\n",
    "\n",
    "    def create_completions(\n",
    "        self,\n",
    "        messages: list[dict[str, str]],\n",
    "        model: str,\n",
    "        temperature: float,\n",
    "        max_tokens: int,\n",
    "        response_format: dict | BaseModel,\n",
    "    ) -> tuple[str | None, str | None]:\n",
    "        try:\n",
    "            response_format = response_format.model_json_schema()\n",
    "        except:\n",
    "            pass\n",
    "        payload = {\n",
    "            \"model\": model,\n",
    "            \"messages\": messages,\n",
    "            \"temperature\": temperature,\n",
    "            \"max_tokens\": max_tokens,\n",
    "            \"format\": response_format,  # Full JSON schema expected here\n",
    "            \"stream\": False\n",
    "        }\n",
    "        self.recent_model = model\n",
    "        headers = deepcopy(self.headers)\n",
    "\n",
    "        for attempt in range(self.retries + 1):\n",
    "            try:\n",
    "                response = requests.post(\n",
    "                    self.endpoint,\n",
    "                    headers=headers,\n",
    "                    json=payload,\n",
    "                    timeout=self.timeout\n",
    "                )\n",
    "                print(response)\n",
    "                response.raise_for_status()\n",
    "                result = response.json()\n",
    "                break\n",
    "            except requests.exceptions.RequestException:\n",
    "                import logging\n",
    "                logging.exception(\"Error on attempt %s calling Ollama endpoint.\", attempt)\n",
    "                result = None\n",
    "\n",
    "        if result is None:\n",
    "            raise Exception(\"Error calling Ollama\")\n",
    "            #return None, None\n",
    "\n",
    "        return self.evaluate_result_path(result), result\n",
    "\n",
    "    def evaluate_result_path(self, raw_result: dict) -> Optional[str]:\n",
    "        if not self.result_path:\n",
    "            return str(raw_result)\n",
    "\n",
    "        try:\n",
    "            keys = self.result_path.replace(\"[\", \".\").replace(\"]\", \"\").split(\".\")\n",
    "            for key in keys:\n",
    "                if isinstance(raw_result, list):\n",
    "                    raw_result = raw_result[int(key)]\n",
    "                else:\n",
    "                    raw_result = raw_result[key]\n",
    "            return str(raw_result)\n",
    "        except (KeyError, ValueError, TypeError):\n",
    "            return None\n",
    "\n",
    "    def unload_model(self) -> None:\n",
    "        \"\"\"\n",
    "        Sends a request to the Ollama API to stop all running models and reset GPU memory.\n",
    "        \"\"\"\n",
    "        if self.recent_model:\n",
    "            stop_url = self.endpoint.replace(\"/api/chat\", \"/api/generate\")\n",
    "            headers = deepcopy(self.headers)\n",
    "\n",
    "            payload = {\n",
    "                \"model\": self.recent_model,\n",
    "                \"keep_alive\": 0\n",
    "            }\n",
    "\n",
    "            try:\n",
    "                response = requests.post(\n",
    "                    stop_url,\n",
    "                    headers=headers,\n",
    "                    json=payload,\n",
    "                    timeout=self.timeout\n",
    "                )\n",
    "                response.raise_for_status()\n",
    "                print(\"Ollama has been successfully reset.\")\n",
    "                sleep(10)\n",
    "            except requests.exceptions.RequestException:\n",
    "                import logging\n",
    "                logging.exception(\"Failed to reset Ollama runtime.\")\n",
    "                raise Exception(\"Error resetting Ollama\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2f06da3",
   "metadata": {},
   "source": [
    "### 3.1.2 Custom Pydantic Template for LLM Output\n",
    "\n",
    "A custom **Pydantic model** defines the expected structure of LLM outputs.\n",
    "It ensures that the extracted property values conform to the predefined data schema and can be validated automatically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "30cc9d06",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PropertyItem(BaseModel):\n",
    "    property: str\n",
    "    value: Union[str, int, float, List[Union[str, int, float]]]\n",
    "    unit: str\n",
    "    reference: str\n",
    "\n",
    "class ExtractionResult(BaseModel):\n",
    "    result: List[PropertyItem]\n",
    "issubclass(ExtractionResult, BaseModel)\n",
    "response_formats = {\n",
    "    \"ExtractionResult\": ExtractionResult\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91258001",
   "metadata": {},
   "source": [
    "### 3.1.3 Custom `pdf2aas` Property Extractor\n",
    "\n",
    "An adapted property extraction class integrates the custom LLM client and output schema.\n",
    "This allows controlled variation of extraction parameters and supports experimental comparison across model configurations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b8233f30",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomPropertyLLMSearch(PropertyLLMSearch):\n",
    "    def extract(\n",
    "        self,\n",
    "        datasheet: list[str] | str,\n",
    "        property_definition: PropertyDefinition | list[PropertyDefinition],\n",
    "        raw_prompts: list | None = None,\n",
    "        raw_results: list | None = None,\n",
    "        prompt_hint: str | None = None,\n",
    "    ) -> list[Property]:\n",
    "        \"\"\"Try to extract all properties found in the given datasheet text.\n",
    "\n",
    "        Ignores the `property_definition` list. Use a more specific PropertyLLM,\n",
    "        e.g. PropertyLLMSearch extractor, if specific property definitions\n",
    "        should be searched.\n",
    "\n",
    "        If a `raw_prompt` or `raw_result` list is given, the created prompts and\n",
    "        returned results are added to these lists.\n",
    "\n",
    "        The `prompt_hint` can be used to add context or additional instructions\n",
    "        to the prompt before it is sent to the LLM.\n",
    "        \"\"\"\n",
    "        if not hasattr(self, \"prompt_degradation_intensity\"):\n",
    "            self.prompt_degradation_intensity = 0.0\n",
    "\n",
    "        if isinstance(datasheet, list):\n",
    "            datasheet = \"\\n\".join(datasheet)\n",
    "\n",
    "        messages = [\n",
    "            {\"role\": \"system\", \"content\": self.system_prompt_template},\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": self.create_prompt(datasheet, property_definition, hint=prompt_hint),\n",
    "            },\n",
    "        ]\n",
    "        if self.prompt_degradation_intensity > 0.0:\n",
    "            messages[0]['content'] = degrade_prompt(messages[0]['content'], self.prompt_degradation_intensity)\n",
    "            messages[1]['content'] = degrade_prompt(messages[1]['content'], self.prompt_degradation_intensity)\n",
    "        if isinstance(raw_prompts, list):\n",
    "            raw_prompts.append(messages)\n",
    "        result = self._prompt_llm(messages, raw_results)\n",
    "        properties = self._parse_result(result)\n",
    "        properties = self._parse_properties(properties)\n",
    "        return self._add_definitions(properties, property_definition)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9b1516e",
   "metadata": {},
   "source": [
    "### 3.1.4 Example LLM Call\n",
    "\n",
    "A short demonstration of a single LLM call is provided to illustrate the interaction between the text prompt, and output model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "250036aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracted 3 properties for 0 definitions.\n"
     ]
    }
   ],
   "source": [
    "use_in_prompt = ['unit','datatype'] # Which information the model gets out of ['definition','unit','datatype', 'values']\n",
    "\n",
    "# Either use Ollama...\n",
    "# client = CustomOllamaClient()\n",
    "# model = \"tinyllama\"\n",
    "\n",
    "# ...or OpenAI\n",
    "client = OpenAI(\n",
    "    api_key=os.environ.get('OPENAI_API_KEY'),\n",
    "    base_url=os.environ.get('OPENAI_BASE_URL')\n",
    " )\n",
    "model = \"gpt-4o-mini\"\n",
    "\n",
    "\n",
    "extractor = CustomPropertyLLMSearch(\n",
    "            model_identifier=model,\n",
    "            client=client,\n",
    "            property_keys_in_prompt=use_in_prompt,\n",
    "        )\n",
    "extractor.prompt_degradation_intensity = 1.0\n",
    "extractor.result_format = response_formats[\"ExtractionResult\"]\n",
    "result = extractor.extract([\"hi\",\"my name is John\",\"I am 30 years old\",\"I live in New York\"], [], [], [],[])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8fa8454",
   "metadata": {},
   "source": [
    "### 3.1.5 Custom Property Dictionary\n",
    "\n",
    "A modified version of the property dictionary use to retrieve templates for the LLM prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a77d20d1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class CustomDictionary(Dictionary):\n",
    "    supported_releases = ['0.0']\n",
    "    def get_class_url(self, class_id: str) -> str | None:\n",
    "        \"\"\"Get the web URL for the class of the class_id for details.\"\"\"\n",
    "        return None\n",
    "\n",
    "    def get_property_url(self, property_id: str) -> str | None:\n",
    "        \"\"\"Get the web URL for the property id for details.\"\"\"\n",
    "        return None\n",
    "\n",
    "dictionary = CustomDictionary(release='0.0')\n",
    "dictionary.load_from_file(\"temp/dict/CustomDictionary-0.0.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fad269bf",
   "metadata": {},
   "source": [
    "## 3.2 Experiment Functions\n",
    "\n",
    "This section defines the experiment orchestration functions, including helper routines for running and logging multiple extraction trials under different degradation conditions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b24c1b90",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_product(product_id, metadata_df, data_path, classification_system=\"ECLASS\"):\n",
    "    product_path = os.path.join(data_path, product_id)\n",
    "    datasheet_path = os.path.join(product_path, product_id + \"_Datasheet.pdf\")\n",
    "    if not os.path.exists(datasheet_path):\n",
    "        if not os.path.exists(os.path.join(product_path, product_id + \".pdf\")):\n",
    "            raise FileNotFoundError(f\"Product folder not found for {product_id}\")\n",
    "        else:\n",
    "            datasheet_path = os.path.join(product_path, product_id + \".pdf\")\n",
    "\n",
    "    if classification_system == \"ECLASS\":\n",
    "        release = str(metadata_df.loc[metadata_df['product_id'] == product_id, 'Classification_System_Version'].iloc[0])\n",
    "        class_id = str(metadata_df.loc[metadata_df['product_id'] == product_id, 'Class_Id'].iloc[0])\n",
    "    elif classification_system == \"CustomDictionary\":\n",
    "        release = \"0.0\"\n",
    "        class_id = product_id\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported classification system: {classification_system}\")\n",
    "    product_classification = {\n",
    "        \"classification_system\": classification_system,\n",
    "        \"class_id\": class_id,\n",
    "        \"release\": release     \n",
    "    }\n",
    "    return datasheet_path, product_classification\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "238c6945",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'43d217f'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Generate short hashes to keep track of different experiment configurations\n",
    "def hash_config(config):\n",
    "    config_str = json.dumps(config, sort_keys=True)\n",
    "    return hashlib.sha1(config_str.encode()).hexdigest()[:8]\n",
    "\n",
    "# Get the current git commit hash to track code version\n",
    "def get_git_commit_hash():\n",
    "    try:\n",
    "        return subprocess.check_output(['git', 'rev-parse', '--short', 'HEAD']).decode('ascii').strip()\n",
    "    except Exception:\n",
    "        return \"unknown\"\n",
    "\n",
    "get_git_commit_hash()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "da4dc2a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_experiment_log(record):\n",
    "    if os.path.exists(log_path):\n",
    "        df = pd.read_csv(log_path)\n",
    "    else:\n",
    "        df = pd.DataFrame()\n",
    "    # Convert record to DataFrame and concatenate\n",
    "    new_record_df = pd.DataFrame([record])\n",
    "    df = pd.concat([df, new_record_df], ignore_index=True)\n",
    "    df.to_csv(log_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "535e3393",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pdf_to_aas(datasheet_path, product_classification, config):\n",
    "    \"\"\"Main Experiment function\"\"\"\n",
    "    \n",
    "    if product_classification['classification_system'] == \"ECLASS\":\n",
    "        dictionary = ECLASS(release=product_classification['release'])\n",
    "    elif product_classification['classification_system'] == \"ETIM\":\n",
    "        dictionary = ETIM(release=product_classification['release'])\n",
    "    elif product_classification['classification_system'] == \"CustomDictionary\":\n",
    "        dictionary = CustomDictionary(release=product_classification['release'])\n",
    "        dictionary.load_from_file(\"temp/dict/CustomDictionary-0.0.json\")\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported classification system: {product_classification['classification_system']}\")\n",
    "    definitions = dictionary.get_class_properties(product_classification['class_id'])\n",
    "    #print(definitions)\n",
    "    dictionary.save_to_file()\n",
    "\n",
    "    # Preprocess the PDF\n",
    "    prep = preprocessor.PDFium()\n",
    "    preprocessed_text = prep.convert(datasheet_path)\n",
    "    preprocessed_datasheet = \"\\n\".join(preprocessed_text) if isinstance(preprocessed_text, list) else preprocessed_text\n",
    "\n",
    "    model = config[\"model\"] if \"model\" in config else \"gpt-4o-mini\"\n",
    "    if config['client'] == \"ollama\":\n",
    "        client = CustomOllamaClient()\n",
    "    else:\n",
    "        client = OpenAI(\n",
    "                api_key=os.environ.get('OPENAI_API_KEY'),\n",
    "                base_url=os.environ.get('OPENAI_BASE_URL')\n",
    "            )       \n",
    "    \n",
    "    \n",
    "    if 'use_in_prompt' in config:\n",
    "        use_in_prompt = config['use_in_prompt']\n",
    "    else:\n",
    "        use_in_prompt = ['unit','datatype'] # any of ['definition','unit','datatype', 'values']\n",
    "    extractor = CustomPropertyLLMSearch(\n",
    "                model_identifier=model,\n",
    "                client=client,\n",
    "                property_keys_in_prompt=use_in_prompt,\n",
    "            )\n",
    "\n",
    "    if \"temperature\" in config:\n",
    "       extractor.temperature = config[\"temperature\"]\n",
    "    if \"response_format\" in config:\n",
    "        extractor.response_format = response_formats[config[\"response_format\"]]\n",
    "\n",
    "    prompt_hint = config[\"prompt_hint\"] if \"prompt_hint\" in config else \"\"\n",
    "    raw_results = []\n",
    "    raw_prompts = []\n",
    "    \n",
    "    if \"prompt_degradation_intensity\" in config:\n",
    "        extractor.prompt_degradation_intensity = config[\"prompt_degradation_intensity\"]\n",
    "    retry = config['retry'] if 'retry' in config else 20\n",
    "     \n",
    "    for i in range(retry):\n",
    "        #print(i)\n",
    "        if 'batch_size' in config:\n",
    "            if config['batch_size'] <= 0:\n",
    "                properties = extractor.extract(\n",
    "                        preprocessed_datasheet,\n",
    "                        property_definition=definitions,\n",
    "                        raw_prompts=raw_prompts,\n",
    "                        prompt_hint=prompt_hint,\n",
    "                        raw_results=raw_results)\n",
    "            elif config['batch_size'] == 1:\n",
    "                properties = []\n",
    "                for d in definitions:\n",
    "                    properties.extend(extractor.extract(\n",
    "                        preprocessed_datasheet,\n",
    "                        property_definition=definitions,\n",
    "                        raw_prompts=raw_prompts,\n",
    "                        prompt_hint=prompt_hint,\n",
    "                        raw_results=raw_results))\n",
    "        else:\n",
    "            properties = []\n",
    "            for i in range(0, len(definitions), config.batch_size):\n",
    "                properties.extend(\n",
    "                    extractor.extract(\n",
    "                    preprocessed_datasheet,\n",
    "                    property_definition=definitions[i : i + config.batch_size],\n",
    "                    raw_prompts=raw_prompts,\n",
    "                    prompt_hint=prompt_hint,\n",
    "                    raw_results=raw_results)\n",
    "                )\n",
    "        #print(properties)\n",
    "        cleaned_properties = []\n",
    "        for prop in properties:\n",
    "            if prop.reference == \"\":\n",
    "                prop.reference = None\n",
    "            if prop.value == \"\":\n",
    "                prop.value = None\n",
    "            cleaned_properties.append(prop)\n",
    "        if len(cleaned_properties) > 1:\n",
    "            break\n",
    "\n",
    "\n",
    "    config_hash = hash_config(config)\n",
    "    now = datetime.now()\n",
    "    date_string = now.strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "    commit_hash = get_git_commit_hash()\n",
    "    submodel_path = os.path.join(processing_path, product_id, f\"{product_id}_pdftoaas_{config_hash}_{date_string}.json\")\n",
    "    config_path = os.path.join(processing_path, \"configs\", f\"config_{config_hash}_{commit_hash}.json\")\n",
    " \n",
    "\n",
    "    #print(submodel_path)\n",
    "    submodel = AASSubmodelTechnicalData()\n",
    "    submodel.add_classification(dictionary, product_classification['class_id'])\n",
    "\n",
    "    submodel.add_properties(cleaned_properties)\n",
    "    submodel.dump(submodel_path)\n",
    "    with open(config_path, \"w\") as f:\n",
    "        json.dump(config, f, indent=4)\n",
    "\n",
    "    record = {\n",
    "        \"product_id\": product_id,\n",
    "        \"date\": date_string,\n",
    "        \"commit_hash\": commit_hash,\n",
    "        \"config_hash\": config_hash,\n",
    "        \"result_path\": submodel_path,\n",
    "        **config\n",
    "    }\n",
    "\n",
    "    update_experiment_log(record)\n",
    "    return client"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caeb59fe",
   "metadata": {},
   "source": [
    "## 3.3 Experiments\n",
    "\n",
    "The core experimental procedures are executed here.\n",
    "Each configuration defines a specific degradation level, prompt style, or model setting.\n",
    "\n",
    "### 3.3.1 Experiment Configurations\n",
    "\n",
    "Configuration dictionaries specify the experimental conditions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e396835a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32\n",
      "[{'classification_system': 'CustomDictionary', 'name': 'deepseek_40960_context', 'model': 'gpt-4o-mini', 'client': 'openai', 'property_keys_in_prompt': ['unit', 'datatype'], 'temperature': 1.2, 'prompt_hint': '', 'prompt_degradation_intensity': 0.0, 'batch_size': 0, 'retry': 20, 'response_format': 'ExtractionResult'}, {'classification_system': 'CustomDictionary', 'name': 'deepseek_40960_context', 'model': 'gpt-4o-mini', 'client': 'openai', 'property_keys_in_prompt': ['unit', 'datatype'], 'temperature': 1.3, 'prompt_hint': '', 'prompt_degradation_intensity': 0.0, 'batch_size': 0, 'retry': 20, 'response_format': 'ExtractionResult'}, {'classification_system': 'CustomDictionary', 'name': 'deepseek_40960_context', 'model': 'gpt-4o-mini', 'client': 'openai', 'property_keys_in_prompt': ['unit', 'datatype'], 'temperature': 1.4, 'prompt_hint': '', 'prompt_degradation_intensity': 0.0, 'batch_size': 0, 'retry': 20, 'response_format': 'ExtractionResult'}, {'classification_system': 'CustomDictionary', 'name': 'deepseek_40960_context', 'model': 'gpt-4o-mini', 'client': 'openai', 'property_keys_in_prompt': ['unit', 'datatype'], 'temperature': 1.5, 'prompt_hint': '', 'prompt_degradation_intensity': 0.0, 'batch_size': 0, 'retry': 20, 'response_format': 'ExtractionResult'}, {'classification_system': 'CustomDictionary', 'name': 'deepseek_40960_context', 'model': 'gpt-4o-mini', 'client': 'openai', 'property_keys_in_prompt': ['unit', 'datatype'], 'temperature': 1.5999999999999999, 'prompt_hint': '', 'prompt_degradation_intensity': 0.0, 'batch_size': 0, 'retry': 20, 'response_format': 'ExtractionResult'}, {'classification_system': 'CustomDictionary', 'name': 'deepseek_40960_context', 'model': 'gpt-4o-mini', 'client': 'openai', 'property_keys_in_prompt': ['unit', 'datatype'], 'temperature': 1.7, 'prompt_hint': '', 'prompt_degradation_intensity': 0.0, 'batch_size': 0, 'retry': 20, 'response_format': 'ExtractionResult'}, {'classification_system': 'CustomDictionary', 'name': 'deepseek_40960_context', 'model': 'gpt-4o-mini', 'client': 'openai', 'property_keys_in_prompt': ['unit', 'datatype'], 'temperature': 1.7999999999999998, 'prompt_hint': '', 'prompt_degradation_intensity': 0.0, 'batch_size': 0, 'retry': 20, 'response_format': 'ExtractionResult'}, {'classification_system': 'CustomDictionary', 'name': 'deepseek_40960_context', 'model': 'gpt-4o-mini', 'client': 'openai', 'property_keys_in_prompt': ['unit', 'datatype'], 'temperature': 1.9, 'prompt_hint': '', 'prompt_degradation_intensity': 0.0, 'batch_size': 0, 'retry': 20, 'response_format': 'ExtractionResult'}, {'classification_system': 'CustomDictionary', 'name': 'deepseek_40960_context', 'model': 'gpt-4o-mini', 'client': 'openai', 'property_keys_in_prompt': ['unit', 'datatype'], 'temperature': 0.0, 'prompt_hint': '', 'prompt_degradation_intensity': 0.0, 'batch_size': 0, 'retry': 20, 'response_format': 'ExtractionResult'}, {'classification_system': 'CustomDictionary', 'name': 'deepseek_40960_context', 'model': 'gpt-4o-mini', 'client': 'openai', 'property_keys_in_prompt': ['unit', 'datatype'], 'temperature': 0.0, 'prompt_hint': '', 'prompt_degradation_intensity': 0.1, 'batch_size': 0, 'retry': 20, 'response_format': 'ExtractionResult'}, {'classification_system': 'CustomDictionary', 'name': 'deepseek_40960_context', 'model': 'gpt-4o-mini', 'client': 'openai', 'property_keys_in_prompt': ['unit', 'datatype'], 'temperature': 0.0, 'prompt_hint': '', 'prompt_degradation_intensity': 0.2, 'batch_size': 0, 'retry': 20, 'response_format': 'ExtractionResult'}, {'classification_system': 'CustomDictionary', 'name': 'deepseek_40960_context', 'model': 'gpt-4o-mini', 'client': 'openai', 'property_keys_in_prompt': ['unit', 'datatype'], 'temperature': 0.0, 'prompt_hint': '', 'prompt_degradation_intensity': 0.30000000000000004, 'batch_size': 0, 'retry': 20, 'response_format': 'ExtractionResult'}, {'classification_system': 'CustomDictionary', 'name': 'deepseek_40960_context', 'model': 'gpt-4o-mini', 'client': 'openai', 'property_keys_in_prompt': ['unit', 'datatype'], 'temperature': 0.0, 'prompt_hint': '', 'prompt_degradation_intensity': 0.4, 'batch_size': 0, 'retry': 20, 'response_format': 'ExtractionResult'}, {'classification_system': 'CustomDictionary', 'name': 'deepseek_40960_context', 'model': 'gpt-4o-mini', 'client': 'openai', 'property_keys_in_prompt': ['unit', 'datatype'], 'temperature': 0.0, 'prompt_hint': '', 'prompt_degradation_intensity': 0.5, 'batch_size': 0, 'retry': 20, 'response_format': 'ExtractionResult'}, {'classification_system': 'CustomDictionary', 'name': 'deepseek_40960_context', 'model': 'gpt-4o-mini', 'client': 'openai', 'property_keys_in_prompt': ['unit', 'datatype'], 'temperature': 0.0, 'prompt_hint': '', 'prompt_degradation_intensity': 0.6000000000000001, 'batch_size': 0, 'retry': 20, 'response_format': 'ExtractionResult'}, {'classification_system': 'CustomDictionary', 'name': 'deepseek_40960_context', 'model': 'gpt-4o-mini', 'client': 'openai', 'property_keys_in_prompt': ['unit', 'datatype'], 'temperature': 0.0, 'prompt_hint': '', 'prompt_degradation_intensity': 0.7000000000000001, 'batch_size': 0, 'retry': 20, 'response_format': 'ExtractionResult'}, {'classification_system': 'CustomDictionary', 'name': 'deepseek_40960_context', 'model': 'gpt-4o-mini', 'client': 'openai', 'property_keys_in_prompt': ['unit', 'datatype'], 'temperature': 0.0, 'prompt_hint': '', 'prompt_degradation_intensity': 0.8, 'batch_size': 0, 'retry': 20, 'response_format': 'ExtractionResult'}, {'classification_system': 'CustomDictionary', 'name': 'deepseek_40960_context', 'model': 'gpt-4o-mini', 'client': 'openai', 'property_keys_in_prompt': ['unit', 'datatype'], 'temperature': 0.0, 'prompt_hint': '', 'prompt_degradation_intensity': 0.9, 'batch_size': 0, 'retry': 20, 'response_format': 'ExtractionResult'}, {'classification_system': 'CustomDictionary', 'name': 'deepseek_40960_context', 'model': 'gpt-4o-mini', 'client': 'openai', 'property_keys_in_prompt': ['unit', 'datatype'], 'temperature': 0.0, 'prompt_hint': '', 'prompt_degradation_intensity': 1.0, 'batch_size': 0, 'retry': 20, 'response_format': 'ExtractionResult'}, {'classification_system': 'CustomDictionary', 'name': 'deepseek_40960_context', 'model': 'deepseek-r1:1.5b', 'client': 'ollama', 'property_keys_in_prompt': ['unit', 'datatype'], 'temperature': 0.0, 'prompt_hint': '', 'prompt_degradation_intensity': 0.0, 'batch_size': 0, 'retry': 20, 'response_format': 'ExtractionResult'}, {'classification_system': 'CustomDictionary', 'name': 'deepseek_40960_context', 'model': 'deepseek-r1:7b', 'client': 'ollama', 'property_keys_in_prompt': ['unit', 'datatype'], 'temperature': 0.0, 'prompt_hint': '', 'prompt_degradation_intensity': 0.0, 'batch_size': 0, 'retry': 20, 'response_format': 'ExtractionResult'}, {'classification_system': 'CustomDictionary', 'name': 'deepseek_40960_context', 'model': 'deepseek-r1:8b', 'client': 'ollama', 'property_keys_in_prompt': ['unit', 'datatype'], 'temperature': 0.0, 'prompt_hint': '', 'prompt_degradation_intensity': 0.0, 'batch_size': 0, 'retry': 20, 'response_format': 'ExtractionResult'}, {'classification_system': 'CustomDictionary', 'name': 'deepseek_40960_context', 'model': 'deepseek-r1:14b', 'client': 'ollama', 'property_keys_in_prompt': ['unit', 'datatype'], 'temperature': 0.0, 'prompt_hint': '', 'prompt_degradation_intensity': 0.0, 'batch_size': 0, 'retry': 20, 'response_format': 'ExtractionResult'}, {'classification_system': 'CustomDictionary', 'name': 'deepseek_40960_context', 'model': 'deepseek-r1:32b', 'client': 'ollama', 'property_keys_in_prompt': ['unit', 'datatype'], 'temperature': 0.0, 'prompt_hint': '', 'prompt_degradation_intensity': 0.0, 'batch_size': 0, 'retry': 20, 'response_format': 'ExtractionResult'}, {'classification_system': 'CustomDictionary', 'name': 'deepseek_40960_context', 'model': 'deepseek-r1:70b', 'client': 'ollama', 'property_keys_in_prompt': ['unit', 'datatype'], 'temperature': 0.0, 'prompt_hint': '', 'prompt_degradation_intensity': 0.0, 'batch_size': 0, 'retry': 20, 'response_format': 'ExtractionResult'}, {'classification_system': 'CustomDictionary', 'name': 'deepseek_40960_context', 'model': 'qwen3:0.6b', 'client': 'ollama', 'property_keys_in_prompt': ['unit', 'datatype'], 'temperature': 0.0, 'prompt_hint': '', 'prompt_degradation_intensity': 0.0, 'batch_size': 0, 'retry': 20, 'response_format': 'ExtractionResult'}, {'classification_system': 'CustomDictionary', 'name': 'deepseek_40960_context', 'model': 'qwen3:1.7b', 'client': 'ollama', 'property_keys_in_prompt': ['unit', 'datatype'], 'temperature': 0.0, 'prompt_hint': '', 'prompt_degradation_intensity': 0.0, 'batch_size': 0, 'retry': 20, 'response_format': 'ExtractionResult'}, {'classification_system': 'CustomDictionary', 'name': 'deepseek_40960_context', 'model': 'qwen3:4b', 'client': 'ollama', 'property_keys_in_prompt': ['unit', 'datatype'], 'temperature': 0.0, 'prompt_hint': '', 'prompt_degradation_intensity': 0.0, 'batch_size': 0, 'retry': 20, 'response_format': 'ExtractionResult'}, {'classification_system': 'CustomDictionary', 'name': 'deepseek_40960_context', 'model': 'qwen3:8b', 'client': 'ollama', 'property_keys_in_prompt': ['unit', 'datatype'], 'temperature': 0.0, 'prompt_hint': '', 'prompt_degradation_intensity': 0.0, 'batch_size': 0, 'retry': 20, 'response_format': 'ExtractionResult'}, {'classification_system': 'CustomDictionary', 'name': 'deepseek_40960_context', 'model': 'qwen3:14b', 'client': 'ollama', 'property_keys_in_prompt': ['unit', 'datatype'], 'temperature': 0.0, 'prompt_hint': '', 'prompt_degradation_intensity': 0.0, 'batch_size': 0, 'retry': 20, 'response_format': 'ExtractionResult'}, {'classification_system': 'CustomDictionary', 'name': 'deepseek_40960_context', 'model': 'qwen3:30b', 'client': 'ollama', 'property_keys_in_prompt': ['unit', 'datatype'], 'temperature': 0.0, 'prompt_hint': '', 'prompt_degradation_intensity': 0.0, 'batch_size': 0, 'retry': 20, 'response_format': 'ExtractionResult'}, {'classification_system': 'CustomDictionary', 'name': 'deepseek_40960_context', 'model': 'qwen3:32b', 'client': 'ollama', 'property_keys_in_prompt': ['unit', 'datatype'], 'temperature': 0.0, 'prompt_hint': '', 'prompt_degradation_intensity': 0.0, 'batch_size': 0, 'retry': 20, 'response_format': 'ExtractionResult'}]\n"
     ]
    }
   ],
   "source": [
    "# generate configurations\n",
    "\n",
    "base_pdftoaas_config = {\n",
    "    \"classification_system\": \"CustomDictionary\", # e.g. \"ECLASS\", \"ETIM\", \"CustomDictionary\"\n",
    "    \"name\": \"deepseek_40960_context\",#\"experiments_test_sample_3\",#\n",
    "    \"model\": \"gpt-4o-mini\", # e.g. \"gpt-4o-mini\", \"llama3:8b\", \"llama3.1:8b\", \"tinyllama\"\n",
    "    \"client\": \"openai\", # e.g. \"openai\", \"ollama\"\n",
    "    \"property_keys_in_prompt\": ['unit','datatype'], # e.g. ['definition','unit','datatype', 'values']\n",
    "    \"temperature\": 0.0,\n",
    "    \"prompt_hint\": \"\",\n",
    "    \"prompt_degradation_intensity\": 0.0, # [0.0-1.0] \n",
    "    \"batch_size\": 0, # 0 = all properties in LLM call\n",
    "    \"retry\": 20,\n",
    "    \"response_format\": \"ExtractionResult\"\n",
    "    #\"commit\": get_git_commit_hash()\n",
    "}\n",
    "\n",
    "# Test runs\n",
    "\n",
    "# temperature\n",
    "temps = np.linspace(1.2, 1.9, 8)\n",
    "configs_temperature = [base_pdftoaas_config | {\"temperature\": float(temp), \"model\": \"gpt-4o-mini\"} for temp in temps]\n",
    "# prompt degradation\n",
    "intensties = np.linspace(0.0, 1.0, 11)\n",
    "configs_prompt_degradation = [base_pdftoaas_config | {\"prompt_degradation_intensity\": float(intensity), \"model\": \"gpt-4o-mini\"} for intensity in intensties]\n",
    "# models\n",
    "deepseek_models = ['deepseek-r1:1.5b','deepseek-r1:7b','deepseek-r1:8b','deepseek-r1:14b','deepseek-r1:32b','deepseek-r1:70b']\n",
    "qwen3_models = ['qwen3:0.6b','qwen3:1.7b','qwen3:4b','qwen3:8b','qwen3:14b','qwen3:30b','qwen3:32b']\n",
    "models = deepseek_models + qwen3_models\n",
    "configs_models = [base_pdftoaas_config | {\"model\": model, \"client\": \"ollama\"} for model in models] \n",
    "\n",
    "configs = configs_temperature + configs_prompt_degradation + configs_models\n",
    "print(len(configs)) # 32 configurations\n",
    "print(configs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5d44901",
   "metadata": {},
   "source": [
    "## 3.3.2 Experiment Loop\n",
    "\n",
    "A main loop iterates through all experiment configurations, executes the extraction pipeline, and records the results.\n",
    "Logs are continuously updated to enable later recovery or analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f25a2fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata_df = pd.read_csv(os.path.join(processing_path, \"metadata.csv\"))\n",
    "recent_client = None\n",
    "recent_model = None\n",
    "\n",
    "\n",
    "\n",
    "for config in configs:\n",
    "    print(config)\n",
    "    # logic for self-hosted ollama models to avoid out of memory errors\n",
    "    if config['model'] != recent_model and config['client'] == 'ollama' and recent_client == 'ollama':\n",
    "        print(\"reloading\")\n",
    "        client.unload_model()\n",
    "\n",
    "\n",
    "\n",
    "    for product_id in os.listdir(processing_path):\n",
    "        product_path = os.path.join(processing_path, product_id)\n",
    "        config_hash = hash_config(config)\n",
    "        pattern = f\"{processing_path}/{product_id}/{product_id}_pdftoaas_{config_hash}_*.json\"\n",
    "        matching_files = glob.glob(pattern)\n",
    "        if not os.path.isdir(product_path) or product_id==\"configs\" or len(matching_files)>0:\n",
    "            continue\n",
    "        print(product_id)\n",
    "        datasheet_path, product_classification = load_product(product_id, metadata_df, processing_path, config['classification_system'])\n",
    "        client = pdf_to_aas(datasheet_path, product_classification, config)\n",
    "\n",
    "        # logic for self-hosted models\n",
    "        recent_model = config['model']\n",
    "        recent_client = config['client']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1f3fd0a",
   "metadata": {},
   "source": [
    "## 3.4 Update Experiment Log from Files\n",
    "\n",
    "This final section updates the experiment logs with existing files and configuration data.\n",
    "It is used to restore experiment states or fill missing log entries after interruptions, such as runtime crashes or manually added output files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4d23e6ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "log_path = os.path.join(processing_path, \"experiment_log.csv\")\n",
    "mylogdf = pd.read_csv(log_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24e0c28b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 6447 records to /app/data/processed/sample/experiment_log.csv\n"
     ]
    }
   ],
   "source": [
    "# Update the experiment logs with existing files and configs in case of missing entries\n",
    "# (e.g. after a crash or if files were added manually)\n",
    "import os\n",
    "import re\n",
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "data_path = \"/app/data/\"\n",
    "processing_path = os.path.join(data_path, \"processed/sample\")\n",
    "log_path = os.path.join(processing_path, \"experiment_log.csv\")\n",
    "configs_path = os.path.join(processing_path, \"configs\")\n",
    "\n",
    "def extract_metadata_from_filename(filename):\n",
    "    # Match pattern like: product_pdftoaas_<config_hash>_<date>.json\n",
    "    match = re.match(r\"(.*?)_pdftoaas_([a-f0-9]+)_(\\d{4}-\\d{2}-\\d{2}_\\d{2}-\\d{2}-\\d{2})\\.json\", filename)\n",
    "    if not match:\n",
    "        return None\n",
    "    product_id, config_hash, date_string = match.groups()\n",
    "    return product_id, config_hash, date_string\n",
    "\n",
    "def migrate_existing_files():\n",
    "    records = []\n",
    "\n",
    "    for root, dirs, files in os.walk(processing_path):\n",
    "        for file in files:\n",
    "            if \"_pdftoaas_\" not in file:\n",
    "                continue\n",
    "\n",
    "            meta = extract_metadata_from_filename(file)\n",
    "            if not meta:\n",
    "                continue\n",
    "\n",
    "            product_id, config_hash, date_string = meta\n",
    "            result_path = os.path.join(root, file)\n",
    "\n",
    "            # Try to find corresponding config\n",
    "            config_filename_pattern = f\"config_{config_hash}_\"\n",
    "            config_file = next((f for f in os.listdir(configs_path) if f.startswith(config_filename_pattern)), None)\n",
    "            if not config_file:\n",
    "                continue\n",
    "\n",
    "            commit_hash = config_file.split(\"_\")[-1].replace(\".json\", \"\")\n",
    "            config_path = os.path.join(configs_path, config_file)\n",
    "\n",
    "            # Load config\n",
    "            with open(config_path, 'r') as f:\n",
    "                config = json.load(f)\n",
    "\n",
    "            record = {\n",
    "                \"product_id\": product_id,\n",
    "                \"date\": date_string,\n",
    "                \"commit_hash\": commit_hash,\n",
    "                \"config_hash\": config_hash,\n",
    "                \"result_path\": result_path,\n",
    "                **config\n",
    "            }\n",
    "            records.append(record)\n",
    "\n",
    "    df = pd.DataFrame(records)\n",
    "    df.to_csv(log_path, index=False)\n",
    "    print(f\"Saved {len(df)} records to {log_path}\")\n",
    "\n",
    "migrate_existing_files()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
